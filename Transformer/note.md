# 编码问题
# 什么是位置编码
位置编码：
在时序任务中，特征与特征间是有顺序关系的，比如NLP中的词与词之间顺序不一致，意思也不一样。
在模型中不仅仅需要考虑特征的表征，还需要考虑位置信息，也就是位置编码。
在RNN\LSTM此类模型中，由于输入即是有序的，所以位置信息实际上是被隐式嵌入了。
在Transformer模型中，token经过embeddeding后是没有位置信息的，是一个对称模型。所以需要使用一个专门的position embedding将位置信息嵌入模型中。

位置信息嵌入需要解决以下几个问题：
1、能够表示一个token在sentence中的绝对位置；
2、在序列长度不同的时候，不同序列中的相对距离也应保持一致。
3、外推。可以用来表示模型在训练时从未见过的句子长度。
4、值有界。

对于第一个问题，可以采用索引法，例如直接以0，1，2，...，n进行编码，但是索引无界，且索引越大数值越大，容易掩盖掉token embeddeding本身的向量，难以收敛。
所以可以采用相对索引，将索引归一到[0,1]之间，避免索引数值过大，但是带来了一个相对距离的问题，相邻两个token之间的position embedding 在不同的sentences中
相对距离不一致。

为解决上述问题，常用的包括：绝对位置编码，相对位置编码。

# 绝对位置编码
绝对位置编码只依靠token的绝对位置来进行编码，一般来说以：Ei+Pi为形式，即token的embedding+token的绝对位置编码Pi
常见的绝对位置编码包括：1、Sinusoidal三角函数编码；2、RoPE螺旋编码(主流大模型常用)
1、Sinusoidal。
    P(k,2i)=sin(k/(10000**(2i/d_model))
    P(k,2i+1)=cos(k/(10000**(2i+1/d_model))
    其中k是token的绝对位置，i是该token的embedding向量的索引。
    即一个sentences表示为[512,1024]，其中512是2k的取值范围，1024是i的取值范围。
    
    问题：是否满足上述位置编码需要解决的几个条件
        对于1：满足。位置信息直接嵌入。
        对于2、4：满足。省去分量来看，P(i)=sin(i) or cos(i), 不依赖于序列长度，且值有界[-1,1]
        对于3：满足。有外推性质，当token数增加时，不影响原有的model稳定性

    问题：只用sin函数行不行，为什么要加cos
        加入cos是为了嵌入相对位置信息。